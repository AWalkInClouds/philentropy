% Generated by roxygen2 (4.1.0): do not edit by hand
% Please edit documentation in R/KL.R
\name{KL}
\alias{KL}
\title{Kullback–Leibler Divergence}
\usage{
KL(P, Q)
}
\arguments{
\item{P}{a probability distribution P.}

\item{Q}{a probability distribution Q.}
}
\value{
The Kullback–Leibler divergence of P and Q.
}
\description{
This function computes the Kullback–Leibler divergence of two probability
distributions P and Q.
}
\details{
\deqn{KL(P||Q) = \sum P(P) * log2(P(P) / P(Q)) = H(P,Q) - H(P)}

where H(P,Q) denotes the \code{\link{JointEntropy}} of the probability
distributions P and Q and H(P) denotes the \code{\link{Entropy}} of
probability distribution P. In case P = Q then KL(P,Q) = 0 and in case P !=
Q then KL(P,Q) > 0.

The KL divergence is a \textbf{non-symmetric measure} of the directed divergence
between two probability distributions P and Q. It only fulfills the
\emph{positivity} property of a \emph{distance metric}.

Because of the relation KL(P||Q) = H(P,Q) - H(P), the Kullback–Leibler
divergence of two probability distributions P and Q is also named
\emph{Cross Entropy} of two probability distributions P and Q.
}
\examples{
# a general example: comparing a normal distribution with an exponential distribution
P <- 1:10/sum(1:10)
Q <- 20:29/sum(20:29)
KLD <- KL(P,Q)

# a phylotranscriptomics example:
# compare the probability distribution of developmental
# stage 1 and 5 of a given PhyloExpressionSet

# read standard phylotranscriptomics data
data(PhyloExpressionSetExample)
data(DivergenceExpressionSetExample)

pKLD <- KL( P = Probability(PhyloExpressionSetExample)[ , 1],
            Q = Probability(PhyloExpressionSetExample)[ , 5] )
}
\author{
Hajk-Georg Drost
}
\references{
Cover Thomas M. and Thomas Joy A. 2006. "Elements of Information
Theory". \emph{John Wiley & Sons}.
}
\seealso{
\code{\link{KL.Matrix}}, \code{\link{Entropy}}, \code{\link{JointEntropy}}
}

