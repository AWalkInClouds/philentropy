<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Information Theory • philentropy</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Information Theory">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">philentropy</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">0.2.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Distances.html">Distance Measures</a>
    </li>
    <li>
      <a href="../articles/Information_Theory.html">Information Theory</a>
    </li>
    <li>
      <a href="../articles/Introduction.html">Introduction</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/HajkD/philentropy">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Information Theory</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/HajkD/philentropy/blob/master/vignettes/Information_Theory.Rmd"><code>vignettes/Information_Theory.Rmd</code></a></small>
      <div class="hidden name"><code>Information_Theory.Rmd</code></div>

    </div>

    
    
<div id="information-theory-measures-in-philentropy" class="section level2">
<h2 class="hasAnchor">
<a href="#information-theory-measures-in-philentropy" class="anchor"></a>Information Theory measures in <code>philentropy</code>
</h2>
<blockquote>
<p>The laws of probability, so true in general, so fallacious in particular.</p>
<p>- Edward Gibbon</p>
</blockquote>
<p>Information theory and statistics were beautifully fused by <code>Solomon Kullback</code>. This fusion allowed to quantify correlations and similarities between random variables using a more sophisticated toolkit. Modern fields such as machine learning and statistical data science build upon this fusion and the most powerful statistical techniques used today are based on an information theoretic foundation.</p>
<p>The <code>philentropy</code> aims to follow this tradition and therefore, it implements the most important information theory measures.</p>
<div id="shannons-entropy-hx" class="section level3">
<h3 class="hasAnchor">
<a href="#shannons-entropy-hx" class="anchor"></a>Shannon’s Entropy H(X)</h3>
<blockquote>
<p><span class="math inline">\(H(X) = -\sum\limits_{i=1}^n P(x_i) * log_b(P(x_i))\)</span></p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define probabilities P(X)</span>
Prob &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
<span class="co"># Compute Shannon's Entropy</span>
<span class="kw"><a href="../reference/H.html">H</a></span>(Prob)</code></pre></div>
<pre><code>[1] 3.103643</code></pre>
</div>
<div id="shannons-joint-entropy-hxy" class="section level3">
<h3 class="hasAnchor">
<a href="#shannons-joint-entropy-hxy" class="anchor"></a>Shannon’s Joint-Entropy H(X,Y)</h3>
<blockquote>
<p><span class="math inline">\(H(X,Y) = -\sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b(P(x_i, y_j))\)</span></p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define the joint distribution P(X,Y)</span>
P_xy &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>)
<span class="co"># Compute Shannon's Joint-Entropy</span>
<span class="kw"><a href="../reference/JE.html">JE</a></span>(P_xy)</code></pre></div>
<pre><code>[1] 6.372236</code></pre>
</div>
<div id="shannons-conditional-entropy-hx-y" class="section level3">
<h3 class="hasAnchor">
<a href="#shannons-conditional-entropy-hx-y" class="anchor"></a>Shannon’s Conditional-Entropy H(X | Y)</h3>
<blockquote>
<p><span class="math inline">\(H(Y|X) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b( P(x_i) / P(x_i, y_j) )\)</span></p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define the distribution P(X)</span>
P_x &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
<span class="co"># define the distribution P(Y)</span>
P_y &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)

<span class="co"># Compute Shannon's Joint-Entropy</span>
<span class="kw"><a href="../reference/CE.html">CE</a></span>(P_x, P_y)</code></pre></div>
<pre><code>[1] 0</code></pre>
</div>
<div id="mutual-information-ixy" class="section level3">
<h3 class="hasAnchor">
<a href="#mutual-information-ixy" class="anchor"></a>Mutual Information I(X,Y)</h3>
<blockquote>
<p><span class="math inline">\(MI(X,Y) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b( P(x_i, y_j) / ( P(x_i) * P(y_j) )\)</span></p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define the distribution P(X)</span>
P_x &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
<span class="co"># define the distribution P(Y)</span>
P_y &lt;-<span class="st"> </span><span class="dv">20</span><span class="op">:</span><span class="dv">29</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">20</span><span class="op">:</span><span class="dv">29</span>)
<span class="co"># define the joint-distribution P(X,Y)</span>
P_xy &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)

<span class="co"># Compute Shannon's Joint-Entropy</span>
<span class="kw"><a href="../reference/MI.html">MI</a></span>(P_x, P_y, P_xy)</code></pre></div>
<pre><code>[1] 3.311973</code></pre>
</div>
<div id="kullback-leibler-divergence" class="section level3">
<h3 class="hasAnchor">
<a href="#kullback-leibler-divergence" class="anchor"></a>Kullback-Leibler Divergence</h3>
<blockquote>
<p><span class="math inline">\(KL(P || Q) = \sum\limits_{i=1}^n P(p_i) * log_2(P(p_i) / P(q_i)) = H(P, Q) - H(P)\)</span></p>
</blockquote>
<p>where <code><a href="../reference/H.html">H(P, Q)</a></code> denotes the joint entropy of the probability distributions <code>P</code> and <code>Q</code> and <code><a href="../reference/H.html">H(P)</a></code> denotes the entropy of probability distribution <code>P</code>. In case <code>P = Q</code> then <code>KL(P, Q) = 0</code> and in case <code>P != Q</code> then <code>KL(P, Q) &gt; 0</code>.</p>
<p>The KL divergence is a non-symmetric measure of the directed divergence between two probability distributions P and Q. It only fulfills the positivity property of a distance metric.</p>
<p>Because of the relation <code>KL(P||Q) = H(P,Q) - H(P)</code>, the Kullback-Leibler divergence of two probability distributions <code>P</code> and <code>Q</code> is also named <code>Cross Entropy</code> of two probability distributions <code>P</code> and <code>Q</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Kulback-Leibler Divergence between random variables P and Q</span>
P &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
Q &lt;-<span class="st"> </span><span class="dv">20</span><span class="op">:</span><span class="dv">29</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">20</span><span class="op">:</span><span class="dv">29</span>)
x &lt;-<span class="st"> </span><span class="kw">rbind</span>(P,Q)

<span class="co"># Kulback-Leibler Divergence between P and Q using different log bases</span>
<span class="kw"><a href="../reference/KL.html">KL</a></span>(x, <span class="dt">unit =</span> <span class="st">"log2"</span>) <span class="co"># Default</span>
<span class="kw"><a href="../reference/KL.html">KL</a></span>(x, <span class="dt">unit =</span> <span class="st">"log"</span>)
<span class="kw"><a href="../reference/KL.html">KL</a></span>(x, <span class="dt">unit =</span> <span class="st">"log10"</span>)</code></pre></div>
<pre><code># KL(x, unit = "log2") # Default
Kulback-Leibler Divergence using unit 'log2'.
kullback-leibler 
       0.1392629 
# KL(x, unit = "log")
Kulback-Leibler Divergence using unit 'log'.
kullback-leibler 
      0.09652967 
# KL(x, unit = "log10")
Kulback-Leibler Divergence using unit 'log10'.
kullback-leibler 
       0.0419223 </code></pre>
</div>
<div id="jensen-shannon-divergence" class="section level3">
<h3 class="hasAnchor">
<a href="#jensen-shannon-divergence" class="anchor"></a>Jensen-Shannon Divergence</h3>
<p>This function computes the <code>Jensen-Shannon Divergence</code> <code><a href="../reference/JSD.html">JSD(P || Q)</a></code> between two probability distributions <code>P</code> and <code>Q</code> with equal weights <code>π_1</code> = <code>π_2</code> = 1/2.</p>
<p>The Jensen-Shannon Divergence JSD(P || Q) between two probability distributions P and Q is defined as:</p>
<blockquote>
<p><span class="math inline">\(JSD(P || Q) = 0.5 * (KL(P || R) + KL(Q || R))\)</span></p>
</blockquote>
<p>where <code>R = 0.5 * (P + Q)</code> denotes the mid-point of the probability vectors <code>P</code> and <code>Q</code>, and <code><a href="../reference/KL.html">KL(P || R)</a></code>, <code><a href="../reference/KL.html">KL(Q || R)</a></code> denote the <code>Kullback-Leibler Divergence</code> of <code>P</code> and <code>R</code>, as well as <code>Q</code> and <code>R</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Jensen-Shannon Divergence between P and Q</span>
P &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
Q &lt;-<span class="st"> </span><span class="dv">20</span><span class="op">:</span><span class="dv">29</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">20</span><span class="op">:</span><span class="dv">29</span>)
x &lt;-<span class="st"> </span><span class="kw">rbind</span>(P,Q)

<span class="co"># Jensen-Shannon Divergence between P and Q using different log bases</span>
<span class="kw"><a href="../reference/JSD.html">JSD</a></span>(x, <span class="dt">unit =</span> <span class="st">"log2"</span>) <span class="co"># Default</span>
<span class="kw"><a href="../reference/JSD.html">JSD</a></span>(x, <span class="dt">unit =</span> <span class="st">"log"</span>)
<span class="kw"><a href="../reference/JSD.html">JSD</a></span>(x, <span class="dt">unit =</span> <span class="st">"log10"</span>)</code></pre></div>
<pre><code># JSD(x, unit = "log2") # Default
Jensen-Shannon Divergence using unit 'log2'.
jensen-shannon 
    0.03792749 
# JSD(x, unit = "log")
Jensen-Shannon Divergence using unit 'log'.
jensen-shannon 
    0.02628933 
# JSD(x, unit = "log10")
Jensen-Shannon Divergence using unit 'log10'.
jensen-shannon 
    0.01141731 </code></pre>
<p>Alternatively, users can specify count data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Jensen-Shannon Divergence Divergence between count vectors P.count and Q.count</span>
P.count &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span>
Q.count &lt;-<span class="st"> </span><span class="dv">20</span><span class="op">:</span><span class="dv">29</span>
x.count &lt;-<span class="st"> </span><span class="kw">rbind</span>(P.count, Q.count)

<span class="kw"><a href="../reference/JSD.html">JSD</a></span>(x, <span class="dt">est.prob =</span> <span class="st">"empirical"</span>)</code></pre></div>
<pre><code>Jensen-Shannon Divergence using unit 'log2'.
jensen-shannon 
    0.03792749</code></pre>
<p>Or users can compute distances based on a probability matrix</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Example: Distance Matrix using JSD-Distance</span>
Prob &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>), <span class="dv">20</span><span class="op">:</span><span class="dv">29</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">20</span><span class="op">:</span><span class="dv">29</span>), <span class="dv">30</span><span class="op">:</span><span class="dv">39</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">30</span><span class="op">:</span><span class="dv">39</span>))

<span class="co"># compute the KL matrix of a given probability matrix</span>
JSDMatrix &lt;-<span class="st"> </span><span class="kw"><a href="../reference/JSD.html">JSD</a></span>(Prob)

JSDMatrix</code></pre></div>
<pre><code>           v1           v2           v3
v1 0.00000000 0.0379274917 0.0435852218
v2 0.03792749 0.0000000000 0.0002120578
v3 0.04358522 0.0002120578 0.0000000000</code></pre>
<div id="properties-of-the-jensen-shannon-divergence" class="section level4">
<h4 class="hasAnchor">
<a href="#properties-of-the-jensen-shannon-divergence" class="anchor"></a>Properties of the <code>Jensen-Shannon Divergence</code>:</h4>
<ul>
<li><p>JSD is non-negative.</p></li>
<li><p>JSD is a symmetric measure JSD(P || Q) = JSD(Q || P).</p></li>
<li><p>JSD = 0, if and only if P = Q.</p></li>
</ul>
</div>
</div>
<div id="generalized-jensen-shannon-divergence" class="section level3">
<h3 class="hasAnchor">
<a href="#generalized-jensen-shannon-divergence" class="anchor"></a>Generalized Jensen-Shannon Divergence</h3>
<p>The generalized Jensen-Shannon Divergence <span class="math inline">\(gJSD_{\pi_1,...,\pi_n}(P_1, ..., P_n)\)</span> enables distance comparisons between multiple probability distributions <span class="math inline">\(P_1,...,P_n\)</span>:</p>
<blockquote>
<p><span class="math inline">\(gJSD_{\pi_1,...,\pi_n}(P_1, ..., P_n) = H(\sum_{i = 1}^n \pi_i*P_i) - \sum_{i = 1}^n \pi_i*H(P_i)\)</span></p>
</blockquote>
<p>where <span class="math inline">\(\pi_1,...,\pi_n\)</span> denote the weights selected for the probability vectors <span class="math inline">\(P_1,...,P_n\)</span> and <span class="math inline">\(H(P_i)\)</span> denotes the Shannon Entropy of probability vector <span class="math inline">\(P_i\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate example probability matrix for comparing three probability functions</span>
Prob &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>), <span class="dv">20</span><span class="op">:</span><span class="dv">29</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">20</span><span class="op">:</span><span class="dv">29</span>), <span class="dv">30</span><span class="op">:</span><span class="dv">39</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">30</span><span class="op">:</span><span class="dv">39</span>))

<span class="co"># compute the Generalized JSD comparing the PS probability matrix</span>
<span class="kw"><a href="../reference/gJSD.html">gJSD</a></span>(Prob)</code></pre></div>
<pre><code>[1] 0.023325</code></pre>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#information-theory-measures-in-philentropy">Information Theory measures in <code>philentropy</code></a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Hajk-Georg Drost.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
