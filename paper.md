---
title: 'Philentropy: Information Theory and Distance Quantification with R'
tags:
  - R
  - information theory
  - distance metrics
  - probability functions
  - divergence quantification
  - jensen-shannon divergence
authors:
  - name: Hajk-Georg Drost
    orcid: 0000-0002-1567-306X
    affiliation: "1"
affiliations:
 - name: The Sainsbury Laboratory, University of Cambridge, Bateman Street, Cambridge CB2 1LR, UK
   index: 1
date: 22 May 2018
bibliography: paper.bib
---

# Summary

Comparison is a fundamental method of scientific research leading to more general insights about the processes that generate similarity or dissimilarity. In statistical terms comparisons between probability functions are performed to infer connections, correlations, or relationships between samples. The philentropy package implements 46 fundamental distance and similarity measures for comparing probability functions. These comparisons between probability functions have their foundations in a broad range of scientific disciplines from mathematics to ecology. The aim of this package is to provide a base framework for clustering, classification, statistical inference, goodness-of-fit, non-parametric statistics, information theory, and machine learning tasks that are based on comparing univeriate or multivariate probability functions.



Citations to entries in paper.bib should be in
[rMarkdown](http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html)
format.


# Acknowledgements

We acknowledge contributions from Brigitta Sipocz, Syrtis Major, and Semyeong
Oh, and support from Kathryn Johnston during the genesis of this project.

# References
